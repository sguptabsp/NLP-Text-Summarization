{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 Lexical Chain\n",
    "# Shruti Gupta (A20381966) and Sagar Mane(A20379756)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load require packages \n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "preprocess\n",
    "\n",
    "Function for preprocessing text file\n",
    "by removing number, stopword, punctuation \n",
    "and non alaphanumeric character\n",
    "\n",
    "Parameter:\n",
    "filename: Name of the read file\n",
    "\n",
    "Return:\n",
    "List of words\n",
    "\n",
    "'''\n",
    "\n",
    "def preprocess(filename):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    tokens = []\n",
    "    words = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for line in open(filename): \n",
    "#         print(line)\n",
    "        all_words = line.split()\n",
    "#         print(all_words)\n",
    "        for w in all_words:\n",
    "            w1 = w\n",
    "            w = stemmer.stem(w)\n",
    "#             w = lemmatiser.lemmatize(w)\n",
    "            if not w.lower() in stop_words and not w in string.punctuation and w.isalpha():\n",
    "                tokens.append(w)\n",
    "                words.append(w1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['girl',\n",
       " 'women',\n",
       " 'person',\n",
       " 'hat',\n",
       " 'skirt',\n",
       " 'girl',\n",
       " 'shirt',\n",
       " 'skirt',\n",
       " 'skirt',\n",
       " 'girl',\n",
       " 'women',\n",
       " 'person',\n",
       " 'shirt',\n",
       " 'rome',\n",
       " 'mumbai',\n",
       " 'one',\n",
       " 'two',\n",
       " 'ten',\n",
       " 'one',\n",
       " 'ten',\n",
       " 'three',\n",
       " 'want',\n",
       " 'told',\n",
       " 'woman']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = preprocess('smalltext.txt')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create lexical chains based on the lexical relationships of synonymy, antonymy, and one level of hyper/hyponymy. In other words, if a noun is related by one of these relationships to an existing lexical chain, it should be added to it, otherwise it should start a new chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getList(word):\n",
    "    list_ = []\n",
    "    w = wordnet.synsets(word)\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            list_.append(l.name())\n",
    "            if l.antonyms():\n",
    "                list_.append(l.antonyms()[0].name())\n",
    "\n",
    "    for i in w[0].hyponyms():\n",
    "        list_.append(i.lemmas()[0].name())\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "getChain\n",
    "Function for finding lexical chain\n",
    "\n",
    "Parameter:\n",
    "tokens: Preprocess words list\n",
    "\n",
    "Return:\n",
    "List of lexical chain\n",
    "\n",
    "'''\n",
    "\n",
    "def getChain(tokens):\n",
    "    list1 = tokens\n",
    "    list2 = tokens\n",
    "    list_ = []\n",
    "    for i in range(len(list1)):\n",
    "        list_.append([])\n",
    "\n",
    "# print(counts)\n",
    "    i=0\n",
    "    for word1 in list1:\n",
    "        for word2 in list2:\n",
    "            w1 = lemmatiser.lemmatize(word1)\n",
    "            w2 = lemmatiser.lemmatize(word2)\n",
    "            wordFromList1 = wordnet.synsets(w1)\n",
    "            wordFromList2 = wordnet.synsets(w2)\n",
    "            if wordFromList1 and wordFromList2: \n",
    "                wup_sim = wordFromList1[0].wup_similarity(wordFromList2[0])\n",
    "                try:\n",
    "                    jcn_sim = wordFromList1[0].jcn_similarity(wordFromList2[0], brown_ic)\n",
    "                except:\n",
    "                    pass\n",
    "#                 print(jcn_sim)\n",
    "                if(wup_sim is not None and wup_sim > 0.6 and jcn_sim > 0.08):\n",
    "                    list_[i].append(word2)\n",
    "#             print (wordFromList1[0],wordFromList2[0],s)\n",
    "        i = i+1\n",
    "\n",
    "# print((list_))\n",
    "\n",
    "    new_k = []\n",
    "    for elem in list_:\n",
    "        if elem not in new_k and len(elem)>0:\n",
    "            new_k.append(elem)\n",
    "    return new_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain 1: woman(1), person(2), girl(3), women(2), \n",
      "Chain 2: skirt(3), hat(1), shirt(2), \n",
      "Chain 3: mumbai(1), rome(1), \n",
      "Chain 4: two(1), three(1), ten(2), one(2), \n",
      "Chain 5: want(1), \n",
      "Chain 6: told(1), \n"
     ]
    }
   ],
   "source": [
    "# Get Lexicon chain list\n",
    "tokens = preprocess('smalltext.txt')\n",
    "k=getChain(tokens)\n",
    "counts = Counter(tokens)\n",
    "n=1\n",
    "for i in k:\n",
    "    l = set(i)\n",
    "    chain = 'Chain '+ str(n) + ': '\n",
    "    for j in l:     \n",
    "        chain = chain + j + \"(\" +  str(counts.get(j)) + \"), \"\n",
    "    n = n + 1\n",
    "    print(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only allow a noun to enter a lexical chain if it is within a threshold number of words (this should be a parameter) to the closest element of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain 1: woman(1), person(2), girl(3), women(2), \n",
      "Chain 2: skirt(3), hat(1), shirt(2), \n",
      "Chain 3: mumbai(1), rome(1), \n",
      "Chain 4: two(1), three(1), ten(2), one(2), \n"
     ]
    }
   ],
   "source": [
    "#Create noun list\n",
    "nouns = {x.name().split('.', 1)[0] for x in wordnet.all_synsets('n')}\n",
    "\n",
    "#Get lixicon chain list\n",
    "tokens = preprocess('smalltext.txt')\n",
    "k=getChain(tokens)\n",
    "counts = Counter(tokens)\n",
    "\n",
    "#Remove Non noun words\n",
    "for i in k:\n",
    "    for j in i:\n",
    "        w1 = lemmatiser.lemmatize(j)\n",
    "        if w1 not in nouns:\n",
    "            i.remove(j)\n",
    "          \n",
    "n=1\n",
    "for i in k:\n",
    "    l = set(i)\n",
    "    if len(l)>0:\n",
    "        chain = 'Chain '+ str(n) + ': '\n",
    "        for j in l: \n",
    "            chain = chain + j + \"(\" +  str(counts.get(j)) + \"), \"\n",
    "        n = n + 1\n",
    "        print(chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra credit: Use the lexical chains to automatically create a summary of the input article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "'''\n",
    "wordScore\n",
    "\n",
    "Function for calculating score of chain\n",
    "\n",
    "Parameter:\n",
    "chains: Lexical chains of similar words\n",
    "tokens: list of preprocess word in text file\n",
    "word: name of word for identify lexical chain\n",
    "\n",
    "Return:\n",
    "Score of lexical chain\n",
    "\n",
    "'''\n",
    "def wordScore(chains,tokens,word):\n",
    "    length = 0\n",
    "    homogeneity = len(set(tokens))/len(tokens)\n",
    "    for chain in chains:\n",
    "        if word in chain:\n",
    "            length = len(chain)\n",
    "    rank = length * homogeneity   \n",
    "   \n",
    "    return rank\n",
    "             \n",
    "'''\n",
    "summarize\n",
    "\n",
    "Function for summarizing large text articles.\n",
    "\n",
    "Parameter: \n",
    "text: Large text file\n",
    "n: no. of senteces in summarize text\n",
    "\n",
    "return:\n",
    "List of summarize text\n",
    "\n",
    "'''\n",
    "def summarize(text, n):\n",
    "    sents = sent_tokenize(text)\n",
    "#     print(sents)\n",
    "#     assert n <= len(sents)\n",
    "    word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "#     print(word_sent)\n",
    "    tokens = []\n",
    "    for s in word_sent:\n",
    "        for w in s:\n",
    "            w = stemmer.stem(w)\n",
    "            if not w.lower() in stop_words and not w in string.punctuation and w.isalpha():\n",
    "                tokens.append(w)\n",
    "    chains = getChain(tokens)\n",
    "\n",
    "#COMPUTING RANKING FOR EACH WORD\n",
    "    _freq = defaultdict(int)\n",
    "    for s in word_sent:\n",
    "        for word in s:\n",
    "            if word not in stop_words:\n",
    "                _freq[word] = wordScore(chains,tokens,word)\n",
    "\n",
    "#COMPTING RANKING FOR EACH SENTENCE\n",
    "    ranking = defaultdict(int)\n",
    "    for i,sent in enumerate(word_sent):\n",
    "        for w in sent:\n",
    "            if w in _freq:\n",
    "                ranking[i] += _freq[w]\n",
    "#     print(ranking)\n",
    "   \n",
    "    sents_idx = nlargest(n, ranking, key=ranking.get)   \n",
    "    return [sents[j] for j in sents_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was Coach Snyder’s first year as our coach.',\n",
       " 'One of the first conversations we had, he came up to me and told me he had watched every game France played in the World Cup.',\n",
       " 'He remembered how we lost to Spain in the group stage — by 24 points — and he remembered how we faced them again in the quarterfinals and won.',\n",
       " 'Spain had the Gasol brothers, Serge Ibaka, Ricky Rubio, a great team.',\n",
       " 'I had just spent the summer playing in the World Cup for the French national team.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = ''\n",
    "for line in open('text.txt'):\n",
    "    lines += line\n",
    "summarize(lines,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
